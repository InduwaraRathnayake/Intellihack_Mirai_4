{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83d135f6",
   "metadata": {
    "papermill": {
     "duration": 0.003122,
     "end_time": "2025-03-09T17:21:50.170634",
     "exception": false,
     "start_time": "2025-03-09T17:21:50.167512",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Stock Price Prediction Challenge ðŸ“ˆ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9104fd20",
   "metadata": {
    "papermill": {
     "duration": 0.002394,
     "end_time": "2025-03-09T17:21:50.176078",
     "exception": false,
     "start_time": "2025-03-09T17:21:50.173684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:2000/1*hhq3ybwbyA3p0dWuLFtLMQ.jpeg\" width=\"600\" height=\"450\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19d6cc74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T17:21:50.184018Z",
     "iopub.status.busy": "2025-03-09T17:21:50.183559Z",
     "iopub.status.idle": "2025-03-09T17:24:24.888185Z",
     "shell.execute_reply": "2025-03-09T17:24:24.886762Z"
    },
    "papermill": {
     "duration": 154.711917,
     "end_time": "2025-03-09T17:24:24.890437",
     "exception": false,
     "start_time": "2025-03-09T17:21:50.178520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Price Prediction Pipeline\n",
      "==============================\n",
      "Loading and preprocessing data...\n",
      "Dataset shape: (11291, 6)\n",
      "Missing values:\n",
      "Adj Close     93\n",
      "Close        117\n",
      "High          95\n",
      "Low          127\n",
      "Open         103\n",
      "Volume       145\n",
      "dtype: int64\n",
      "Missing values after handling:\n",
      "Adj Close    0\n",
      "Close        0\n",
      "High         0\n",
      "Low          0\n",
      "Open         0\n",
      "Volume       0\n",
      "dtype: int64\n",
      "\n",
      "Checking for problematic values at stage: after loading data\n",
      "Total NaN values: 0\n",
      "Total infinity values: 0\n",
      "\n",
      "Performing Exploratory Data Analysis...\n",
      "Augmented Dickey-Fuller Test:\n",
      "ADF Statistic: -0.467371512452141\n",
      "p-value: 0.8982279985700206\n",
      "\n",
      "Engineering features...\n",
      "\n",
      "Checking for problematic values at stage: after feature engineering, before cleaning\n",
      "Total NaN values: 638\n",
      "Columns with NaN values:\n",
      "Daily_Return       1\n",
      "MA5                4\n",
      "MA10               9\n",
      "MA20              19\n",
      "MA50              49\n",
      "MA200            199\n",
      "RSI               13\n",
      "MA20_std          19\n",
      "Upper_Band        19\n",
      "Lower_Band        19\n",
      "BB_Width          19\n",
      "Momentum           5\n",
      "ROC                5\n",
      "ATR               13\n",
      "Lag_1              1\n",
      "Lag_2              2\n",
      "Lag_3              3\n",
      "Lag_4              4\n",
      "Lag_5              5\n",
      "Volume_MA5         4\n",
      "Volume_Change      1\n",
      "Target             5\n",
      "Day_of_Week      110\n",
      "Month            110\n",
      "dtype: int64\n",
      "Total infinity values: 0\n",
      "Columns with extremely large values (>1e10): ['Price_Change_Pct', 'Volume_Change']\n",
      "\n",
      "Checking for problematic values at stage: after feature engineering and cleaning\n",
      "Total NaN values: 0\n",
      "Total infinity values: 0\n",
      "Columns with extremely large values (>1e10): ['Price_Change_Pct', 'Volume_Change']\n",
      "Shape after feature engineering and dropping NAs: (10980, 39)\n",
      "\n",
      "Selecting features...\n",
      "Top 10 features:\n",
      "       Feature  Importance\n",
      "1        Close    0.482902\n",
      "2         High    0.198913\n",
      "0    Adj Close    0.130008\n",
      "3          Low    0.101695\n",
      "4         Open    0.060120\n",
      "6          MA5    0.007709\n",
      "24       Lag_1    0.005858\n",
      "11       EMA12    0.005225\n",
      "7         MA10    0.001940\n",
      "18  Upper_Band    0.001158\n",
      "\n",
      "Training and evaluating models...\n",
      "Evaluating Linear Regression...\n",
      "Evaluating Random Forest...\n",
      "Evaluating Gradient Boosting...\n",
      "Evaluating XGBoost...\n",
      "\n",
      "Model Comparison:\n",
      "                           RMSE           MAE            RÂ²  \\\n",
      "Linear Regression  4.684705e+07  2.056659e+06 -5.508687e+13   \n",
      "Random Forest      1.546172e+01  1.078911e+01  1.193477e-02   \n",
      "Gradient Boosting  1.572879e+01  1.085283e+01  1.002540e-02   \n",
      "XGBoost            1.668618e+01  1.173887e+01 -1.026625e-01   \n",
      "\n",
      "                   Direction Accuracy  \n",
      "Linear Regression            0.505902  \n",
      "Random Forest                0.500874  \n",
      "Gradient Boosting            0.507541  \n",
      "XGBoost                      0.505027  \n",
      "\n",
      "Best model based on Direction Accuracy: Gradient Boosting\n",
      "\n",
      "Evaluating trading strategy with Gradient Boosting...\n",
      "Overall Direction Accuracy: 0.4791\n",
      "\n",
      "Trading Strategy Performance:\n",
      "Total Return: -0.9728\n",
      "Market Return: 36.3441\n",
      "Annualized Return: -0.0794\n",
      "Sharpe Ratio: -0.2233\n",
      "Maximum Drawdown: -0.9957\n",
      "Win Rate: 0.4505\n",
      "\n",
      "Training final Gradient Boosting model...\n",
      "\n",
      "Generating predictions for future days...\n",
      "Predictions saved to 'stock_predictions.csv'\n",
      "\n",
      "Generating README file...\n",
      "README.md generated successfully\n",
      "\n",
      "Stock prediction pipeline completed successfully!\n",
      "Check the generated files for results and visualizations.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Load and preprocess the data\n",
    "def load_and_preprocess_data(file_path):\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Remove 'Unnamed: 0' column if it exists\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df = df.drop('Unnamed: 0', axis=1)\n",
    "    \n",
    "    # Convert Date to datetime and set as index\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Display basic information\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Missing values:\\n{df.isnull().sum()}\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    # For price data, use forward fill followed by backward fill\n",
    "    for col in ['Open', 'High', 'Low', 'Close', 'Adj Close']:\n",
    "        df[col] = df[col].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # For volume, use median\n",
    "    df['Volume'] = df['Volume'].fillna(df['Volume'].median())\n",
    "    \n",
    "    print(f\"Missing values after handling:\\n{df.isnull().sum()}\")\n",
    "    return df\n",
    "    \n",
    "    # Convert Date to datetime and set as index\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Display basic information\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Missing values:\\n{df.isnull().sum()}\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    # For price data, use forward fill followed by backward fill\n",
    "    for col in ['Open', 'High', 'Low', 'Close', 'Adj Close']:\n",
    "        df[col] = df[col].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # For volume, use median\n",
    "    df['Volume'] = df['Volume'].fillna(df['Volume'].median())\n",
    "    \n",
    "    print(f\"Missing values after handling:\\n{df.isnull().sum()}\")\n",
    "    return df\n",
    "\n",
    "# 2. Exploratory Data Analysis\n",
    "def perform_eda(df):\n",
    "    print(\"\\nPerforming Exploratory Data Analysis...\")\n",
    "    # Plot the closing price\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df['Close'])\n",
    "    plt.title('Stock Closing Price Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('closing_price_over_time.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Calculate daily returns\n",
    "    df['Daily_Return'] = df['Close'].pct_change()\n",
    "    \n",
    "    # Plot the daily returns\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df['Daily_Return'])\n",
    "    plt.title('Daily Returns Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Return')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('daily_returns.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Distribution of daily returns\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(df['Daily_Return'].dropna(), kde=True)\n",
    "    plt.title('Distribution of Daily Returns')\n",
    "    plt.xlabel('Daily Return')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('return_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Check for stationarity\n",
    "    result = adfuller(df['Close'].dropna())\n",
    "    print('Augmented Dickey-Fuller Test:')\n",
    "    print(f'ADF Statistic: {result[0]}')\n",
    "    print(f'p-value: {result[1]}')\n",
    "    \n",
    "    # Correlation matrix\n",
    "    correlation = df.corr()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation, annot=True, cmap='coolwarm')\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.savefig('correlation_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Try to perform seasonal decomposition if enough data\n",
    "    try:\n",
    "        # Decompose the time series (using a subset if too large)\n",
    "        if len(df) > 1000:\n",
    "            sample_df = df[-1000:].copy()\n",
    "        else:\n",
    "            sample_df = df.copy()\n",
    "            \n",
    "        result = seasonal_decompose(sample_df['Close'], model='multiplicative', period=252)\n",
    "        \n",
    "        # Plot decomposition\n",
    "        fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 12))\n",
    "        result.observed.plot(ax=ax1)\n",
    "        ax1.set_title('Observed')\n",
    "        result.trend.plot(ax=ax2)\n",
    "        ax2.set_title('Trend')\n",
    "        result.seasonal.plot(ax=ax3)\n",
    "        ax3.set_title('Seasonality')\n",
    "        result.resid.plot(ax=ax4)\n",
    "        ax4.set_title('Residuals')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('seasonal_decomposition.png')\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not perform seasonal decomposition: {e}\")\n",
    "    \n",
    "    return df\n",
    "# Add this function to detect and report problematic values in your dataset\n",
    "def check_for_problematic_values(df, stage_name=\"\"):\n",
    "    \"\"\"Check for NaN, infinity or extremely large values in the dataframe.\"\"\"\n",
    "    print(f\"\\nChecking for problematic values at stage: {stage_name}\")\n",
    "    \n",
    "    # Check for NaN values\n",
    "    nan_count = df.isna().sum().sum()\n",
    "    print(f\"Total NaN values: {nan_count}\")\n",
    "    if nan_count > 0:\n",
    "        print(\"Columns with NaN values:\")\n",
    "        print(df.isna().sum()[df.isna().sum() > 0])\n",
    "    \n",
    "    # Check for infinity values\n",
    "    inf_count = np.isinf(df.select_dtypes(include=[np.number])).sum().sum()\n",
    "    print(f\"Total infinity values: {inf_count}\")\n",
    "    if inf_count > 0:\n",
    "        inf_cols = df.columns[np.isinf(df.select_dtypes(include=[np.number])).any()]\n",
    "        print(f\"Columns with infinity values: {list(inf_cols)}\")\n",
    "    \n",
    "    # Check for extremely large values\n",
    "    max_values = df.select_dtypes(include=[np.number]).max()\n",
    "    large_value_cols = max_values[max_values > 1e10].index.tolist()\n",
    "    if large_value_cols:\n",
    "        print(f\"Columns with extremely large values (>1e10): {large_value_cols}\")\n",
    "    \n",
    "    return nan_count > 0 or inf_count > 0 or bool(large_value_cols)\n",
    "\n",
    "# Modify the engineer_features function to handle division by zero\n",
    "def engineer_features(df):\n",
    "    print(\"\\nEngineering features...\")\n",
    "    # Moving averages\n",
    "    df['MA5'] = df['Close'].rolling(window=5).mean()\n",
    "    df['MA10'] = df['Close'].rolling(window=10).mean()\n",
    "    df['MA20'] = df['Close'].rolling(window=20).mean()\n",
    "    df['MA50'] = df['Close'].rolling(window=50).mean()\n",
    "    df['MA200'] = df['Close'].rolling(window=200).mean()\n",
    "    \n",
    "    # Exponential moving averages\n",
    "    df['EMA12'] = df['Close'].ewm(span=12, adjust=False).mean()\n",
    "    df['EMA26'] = df['Close'].ewm(span=26, adjust=False).mean()\n",
    "    \n",
    "    # MACD\n",
    "    df['MACD'] = df['EMA12'] - df['EMA26']\n",
    "    df['Signal_Line'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "    df['MACD_Histogram'] = df['MACD'] - df['Signal_Line']\n",
    "    \n",
    "    # RSI - Modified to avoid division by zero\n",
    "    delta = df['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    # Avoid division by zero by adding a small value\n",
    "    rs = gain / (loss + 1e-10)  # Add small epsilon to prevent division by zero\n",
    "    df['RSI'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Bollinger Bands - Modified to handle zero standard deviation\n",
    "    df['MA20_std'] = df['Close'].rolling(window=20).std()\n",
    "    # Replace zero std with a small value to avoid division by zero\n",
    "    df['MA20_std'] = df['MA20_std'].replace(0, 1e-10)\n",
    "    \n",
    "    df['Upper_Band'] = df['MA20'] + (df['MA20_std'] * 2)\n",
    "    df['Lower_Band'] = df['MA20'] - (df['MA20_std'] * 2)\n",
    "    # Avoid division by zero in BB_Width\n",
    "    df['BB_Width'] = (df['Upper_Band'] - df['Lower_Band']) / (df['MA20'] + 1e-10)\n",
    "    \n",
    "    # Momentum and Rate of Change\n",
    "    df['Momentum'] = df['Close'] - df['Close'].shift(5)\n",
    "    # Avoid division by zero in ROC\n",
    "    df['ROC'] = df['Close'] / (df['Close'].shift(5) + 1e-10) - 1\n",
    "    df['ROC'] = df['ROC'] * 100\n",
    "    \n",
    "    # Average True Range (ATR)\n",
    "    high_low = df['High'] - df['Low']\n",
    "    high_close = np.abs(df['High'] - df['Close'].shift())\n",
    "    low_close = np.abs(df['Low'] - df['Close'].shift())\n",
    "    ranges = pd.concat([high_low, high_close, low_close], axis=1)\n",
    "    true_range = np.max(ranges, axis=1)\n",
    "    df['ATR'] = true_range.rolling(14).mean()\n",
    "    \n",
    "    # Lag features\n",
    "    for i in range(1, 6):\n",
    "        df[f'Lag_{i}'] = df['Close'].shift(i)\n",
    "    \n",
    "    # Price change features\n",
    "    df['Price_Change'] = df['Close'] - df['Open']\n",
    "    # Avoid division by zero in Price_Change_Pct\n",
    "    df['Price_Change_Pct'] = (df['Close'] - df['Open']) / (df['Open'] + 1e-10) * 100\n",
    "    \n",
    "    # Volume indicators\n",
    "    df['Volume_MA5'] = df['Volume'].rolling(window=5).mean()\n",
    "    # Avoid division by zero in Volume_Change\n",
    "    df['Volume_Change'] = df['Volume'] / (df['Volume'].shift(1) + 1e-10) - 1\n",
    "    \n",
    "    # Target variable: price 5 days ahead\n",
    "    df['Target'] = df['Close'].shift(-5)\n",
    "    \n",
    "    # High/Low diff\n",
    "    df['HL_Diff'] = df['High'] - df['Low']\n",
    "    # Avoid division by zero in HL_Pct\n",
    "    df['HL_Pct'] = df['HL_Diff'] / (df['Low'] + 1e-10) * 100\n",
    "    \n",
    "    # Day of week feature\n",
    "    df['Day_of_Week'] = df.index.dayofweek\n",
    "    \n",
    "    # Month feature\n",
    "    df['Month'] = df.index.month\n",
    "    \n",
    "    # Check for problems before cleaning\n",
    "    has_problems = check_for_problematic_values(df, \"after feature engineering, before cleaning\")\n",
    "    \n",
    "    # Replace infinity values with NaN\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    # Drop missing values after feature creation\n",
    "    df_complete = df.dropna().copy()\n",
    "    \n",
    "    # Optional: For features that might still have extreme values, clip them\n",
    "    # For each numeric column, clip values to reasonable bounds\n",
    "    # Define a function to clip values based on their distribution\n",
    "    def clip_extreme_values(series, n_std=5):\n",
    "        if pd.api.types.is_numeric_dtype(series):\n",
    "            mean = series.mean()\n",
    "            std = series.std()\n",
    "            lower_bound = mean - n_std * std\n",
    "            upper_bound = mean + n_std * std\n",
    "            return series.clip(lower_bound, upper_bound)\n",
    "        return series\n",
    "    \n",
    "    # Apply clipping to all numeric columns\n",
    "    for col in df_complete.select_dtypes(include=[np.number]).columns:\n",
    "        if col not in ['Day_of_Week', 'Month']:  # Skip categorical numeric columns\n",
    "            df_complete[col] = clip_extreme_values(df_complete[col])\n",
    "    \n",
    "    # Final check after cleaning\n",
    "    check_for_problematic_values(df_complete, \"after feature engineering and cleaning\")\n",
    "    \n",
    "    print(f\"Shape after feature engineering and dropping NAs: {df_complete.shape}\")\n",
    "    return df_complete\n",
    "\n",
    "# Modify the train_and_evaluate_models function to handle errors\n",
    "def train_and_evaluate_models(df, features):\n",
    "    print(\"\\nTraining and evaluating models...\")\n",
    "    # Prepare data for modeling\n",
    "    X = df[features].copy()\n",
    "    y = df['Target'].copy()\n",
    "    \n",
    "    # Use time series cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    # Define models to test\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "        'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42, objective='reg:squarederror')\n",
    "    }\n",
    "    \n",
    "    # Compare models\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        print(f\"Evaluating {name}...\")\n",
    "        rmse_scores = []\n",
    "        mae_scores = []\n",
    "        r2_scores = []\n",
    "        direction_accuracy = []\n",
    "        \n",
    "        for train_idx, test_idx in tscv.split(X):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "            \n",
    "            # Additional safety check for problematic values\n",
    "            X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "            X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "            \n",
    "            # Fill any remaining NaN with appropriate values\n",
    "            X_train.fillna(X_train.median(), inplace=True)\n",
    "            X_test.fillna(X_train.median(), inplace=True)  # Use training median for test data\n",
    "            \n",
    "            # Scale features with robust handling\n",
    "            try:\n",
    "                scaler = StandardScaler()\n",
    "                X_train_scaled = scaler.fit_transform(X_train)\n",
    "                X_test_scaled = scaler.transform(X_test)\n",
    "                \n",
    "                # Quick check for any remaining infinity values\n",
    "                if np.any(np.isinf(X_train_scaled)) or np.any(np.isnan(X_train_scaled)):\n",
    "                    print(f\"Warning: Infinity or NaN values in X_train_scaled for {name}\")\n",
    "                    # Replace any remaining problematic values\n",
    "                    X_train_scaled = np.nan_to_num(X_train_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                \n",
    "                if np.any(np.isinf(X_test_scaled)) or np.any(np.isnan(X_test_scaled)):\n",
    "                    print(f\"Warning: Infinity or NaN values in X_test_scaled for {name}\")\n",
    "                    # Replace any remaining problematic values\n",
    "                    X_test_scaled = np.nan_to_num(X_test_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                \n",
    "                # Train model with error handling\n",
    "                try:\n",
    "                    model.fit(X_train_scaled, y_train)\n",
    "                    \n",
    "                    # Make predictions\n",
    "                    y_pred = model.predict(X_test_scaled)\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "                    mae = mean_absolute_error(y_test, y_pred)\n",
    "                    r2 = r2_score(y_test, y_pred)\n",
    "                    \n",
    "                    # Direction accuracy\n",
    "                    actual_direction = np.sign(y_test.values - df['Close'].iloc[test_idx].values)\n",
    "                    pred_direction = np.sign(y_pred - df['Close'].iloc[test_idx].values)\n",
    "                    direction_acc = np.mean(actual_direction == pred_direction)\n",
    "                    \n",
    "                    rmse_scores.append(rmse)\n",
    "                    mae_scores.append(mae)\n",
    "                    r2_scores.append(r2)\n",
    "                    direction_accuracy.append(direction_acc)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error during model training/evaluation for {name}: {e}\")\n",
    "                    # Add placeholder values if model fails\n",
    "                    rmse_scores.append(float('inf'))\n",
    "                    mae_scores.append(float('inf'))\n",
    "                    r2_scores.append(0)\n",
    "                    direction_accuracy.append(0)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error during scaling for {name}: {e}\")\n",
    "                # Add placeholder values if scaling fails\n",
    "                rmse_scores.append(float('inf'))\n",
    "                mae_scores.append(float('inf'))\n",
    "                r2_scores.append(0)\n",
    "                direction_accuracy.append(0)\n",
    "        \n",
    "        # Only calculate mean metrics if we have valid scores\n",
    "        valid_scores = [score for score in rmse_scores if score != float('inf')]\n",
    "        if valid_scores:\n",
    "            results[name] = {\n",
    "                'RMSE': np.mean(valid_scores),\n",
    "                'MAE': np.mean([score for score in mae_scores if score != float('inf')]),\n",
    "                'RÂ²': np.mean([score for score in r2_scores if score != 0]),\n",
    "                'Direction Accuracy': np.mean([score for score in direction_accuracy if score != 0])\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Warning: No valid evaluation scores for {name}. Using placeholders.\")\n",
    "            results[name] = {\n",
    "                'RMSE': float('inf'),\n",
    "                'MAE': float('inf'),\n",
    "                'RÂ²': 0,\n",
    "                'Direction Accuracy': 0\n",
    "            }\n",
    "    \n",
    "    # Display results\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df.to_csv('model_comparison_results.csv')\n",
    "    \n",
    "    # Select best model based on direction accuracy and RMSE\n",
    "    # First check if we have any valid models\n",
    "    valid_models = results_df[results_df['RMSE'] < float('inf')].index.tolist()\n",
    "    \n",
    "    if not valid_models:\n",
    "        print(\"No model succeeded in training. Using Linear Regression as fallback.\")\n",
    "        best_model_name = 'Linear Regression'\n",
    "    else:\n",
    "        best_model_name = results_df.loc[valid_models, 'Direction Accuracy'].idxmax()\n",
    "        print(f\"\\nBest model based on Direction Accuracy: {best_model_name}\")\n",
    "        \n",
    "        # In case there's a tie or very close scores, consider RMSE too\n",
    "        if results_df.loc[best_model_name, 'RMSE'] > results_df.loc[valid_models, 'RMSE'].min() * 1.1:\n",
    "            best_model_name = results_df.loc[valid_models, 'RMSE'].idxmin()\n",
    "            print(f\"Selecting model with lowest RMSE instead: {best_model_name}\")\n",
    "    \n",
    "    return best_model_name, models[best_model_name], results_df\n",
    "\n",
    "\n",
    "# 4. Feature Selection\n",
    "def select_features(df):\n",
    "    print(\"\\nSelecting features...\")\n",
    "    # Prepare data for feature importance\n",
    "    X = df.drop(['Target', 'Daily_Return'], axis=1)\n",
    "    y = df['Target']\n",
    "    \n",
    "    # Use Random Forest to get feature importance\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance.head(15)\n",
    "    sns.barplot(x='Importance', y='Feature', data=top_features)\n",
    "    plt.title('Top 15 Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Top 10 features:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Select top features (you can adjust the number)\n",
    "    top_features_list = feature_importance['Feature'].tolist()\n",
    "    \n",
    "    return top_features_list\n",
    "\n",
    "# 5. Model Training and Evaluation\n",
    "def train_and_evaluate_models(df, features):\n",
    "    print(\"\\nTraining and evaluating models...\")\n",
    "    # Prepare data for modeling\n",
    "    X = df[features]\n",
    "    y = df['Target']\n",
    "    \n",
    "    # Use time series cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    # Define models to test\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "        'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42, objective='reg:squarederror')\n",
    "    }\n",
    "    \n",
    "    # Compare models\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        print(f\"Evaluating {name}...\")\n",
    "        rmse_scores = []\n",
    "        mae_scores = []\n",
    "        r2_scores = []\n",
    "        direction_accuracy = []\n",
    "        \n",
    "        for train_idx, test_idx in tscv.split(X):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "            \n",
    "            # Scale features\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Train model\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            # Direction accuracy\n",
    "            actual_direction = np.sign(y_test.values - df['Close'].iloc[test_idx].values)\n",
    "            pred_direction = np.sign(y_pred - df['Close'].iloc[test_idx].values)\n",
    "            direction_acc = np.mean(actual_direction == pred_direction)\n",
    "            \n",
    "            rmse_scores.append(rmse)\n",
    "            mae_scores.append(mae)\n",
    "            r2_scores.append(r2)\n",
    "            direction_accuracy.append(direction_acc)\n",
    "        \n",
    "        results[name] = {\n",
    "            'RMSE': np.mean(rmse_scores),\n",
    "            'MAE': np.mean(mae_scores),\n",
    "            'RÂ²': np.mean(r2_scores),\n",
    "            'Direction Accuracy': np.mean(direction_accuracy)\n",
    "        }\n",
    "    \n",
    "    # Display results\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df.to_csv('model_comparison_results.csv')\n",
    "    \n",
    "    # Select best model based on direction accuracy and RMSE\n",
    "    best_model_name = results_df['Direction Accuracy'].idxmax()\n",
    "    print(f\"\\nBest model based on Direction Accuracy: {best_model_name}\")\n",
    "    \n",
    "    # In case there's a tie or very close scores, consider RMSE too\n",
    "    if results_df.loc[best_model_name, 'RMSE'] > results_df['RMSE'].min() * 1.1:\n",
    "        best_model_name = results_df['RMSE'].idxmin()\n",
    "        print(f\"Selecting model with lowest RMSE instead: {best_model_name}\")\n",
    "    \n",
    "    return best_model_name, models[best_model_name], results_df\n",
    "\n",
    "# 6. Trading Strategy Evaluation\n",
    "def evaluate_trading_strategy(df, model_name, model, features):\n",
    "    print(f\"\\nEvaluating trading strategy with {model_name}...\")\n",
    "    \n",
    "    # Create a copy of the dataframe for evaluation\n",
    "    eval_df = df.copy()\n",
    "    \n",
    "    # Prepare features\n",
    "    X = eval_df[features]\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Make predictions\n",
    "    eval_df['Predicted_Price'] = model.predict(X_scaled)\n",
    "    eval_df['Actual_Direction'] = np.sign(eval_df['Target'] - eval_df['Close'])\n",
    "    eval_df['Predicted_Direction'] = np.sign(eval_df['Predicted_Price'] - eval_df['Close'])\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    eval_df['Correct_Prediction'] = (eval_df['Actual_Direction'] == eval_df['Predicted_Direction']).astype(int)\n",
    "    direction_accuracy = eval_df['Correct_Prediction'].mean()\n",
    "    print(f\"Overall Direction Accuracy: {direction_accuracy:.4f}\")\n",
    "    \n",
    "    # Simple trading strategy: Buy when predicted price is higher, sell when lower\n",
    "    eval_df['Position'] = eval_df['Predicted_Direction'].shift(1)\n",
    "    eval_df['Position'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Calculate returns\n",
    "    eval_df['Strategy_Return'] = eval_df['Position'] * eval_df['Daily_Return']\n",
    "    \n",
    "    # Calculate cumulative returns\n",
    "    eval_df['Cumulative_Market_Return'] = (1 + eval_df['Daily_Return']).cumprod() - 1\n",
    "    eval_df['Cumulative_Strategy_Return'] = (1 + eval_df['Strategy_Return']).cumprod() - 1\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    total_return = eval_df['Cumulative_Strategy_Return'].iloc[-1]\n",
    "    market_return = eval_df['Cumulative_Market_Return'].iloc[-1]\n",
    "    \n",
    "    # Calculate annualized return (assuming 252 trading days)\n",
    "    years = len(eval_df) / 252\n",
    "    annualized_return = (1 + total_return) ** (1 / years) - 1\n",
    "    \n",
    "    # Calculate Sharpe ratio (assuming 252 trading days)\n",
    "    risk_free_rate = 0.02 / 252  # Assuming 2% annual risk-free rate\n",
    "    sharpe_ratio = np.sqrt(252) * (eval_df['Strategy_Return'].mean() - risk_free_rate) / eval_df['Strategy_Return'].std()\n",
    "    \n",
    "    # Calculate maximum drawdown\n",
    "    cum_returns = (1 + eval_df['Strategy_Return']).cumprod()\n",
    "    running_max = cum_returns.cummax()\n",
    "    drawdown = (cum_returns / running_max) - 1\n",
    "    max_drawdown = drawdown.min()\n",
    "    \n",
    "    # Calculate win rate\n",
    "    win_rate = (eval_df['Strategy_Return'] > 0).mean()\n",
    "    \n",
    "    # Plot cumulative returns\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(eval_df.index, eval_df['Cumulative_Market_Return'], label='Buy and Hold')\n",
    "    plt.plot(eval_df.index, eval_df['Cumulative_Strategy_Return'], label='Trading Strategy')\n",
    "    plt.title('Cumulative Returns: Strategy vs Buy-and-Hold')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cumulative Return')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('strategy_performance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Print performance metrics\n",
    "    print(\"\\nTrading Strategy Performance:\")\n",
    "    print(f\"Total Return: {total_return:.4f}\")\n",
    "    print(f\"Market Return: {market_return:.4f}\")\n",
    "    print(f\"Annualized Return: {annualized_return:.4f}\")\n",
    "    print(f\"Sharpe Ratio: {sharpe_ratio:.4f}\")\n",
    "    print(f\"Maximum Drawdown: {max_drawdown:.4f}\")\n",
    "    print(f\"Win Rate: {win_rate:.4f}\")\n",
    "    \n",
    "    # Create a dataframe with performance metrics\n",
    "    performance = {\n",
    "        'Total Return': total_return,\n",
    "        'Market Return': market_return,\n",
    "        'Annualized Return': annualized_return,\n",
    "        'Sharpe Ratio': sharpe_ratio,\n",
    "        'Maximum Drawdown': max_drawdown,\n",
    "        'Win Rate': win_rate,\n",
    "        'Direction Accuracy': direction_accuracy\n",
    "    }\n",
    "    \n",
    "    performance_df = pd.DataFrame([performance])\n",
    "    performance_df.to_csv('trading_strategy_performance.csv', index=False)\n",
    "    \n",
    "    return performance, eval_df\n",
    "\n",
    "# 7. Final Model Training\n",
    "def train_final_model(df, model_name, features):\n",
    "    print(f\"\\nTraining final {model_name} model...\")\n",
    "    # Prepare data\n",
    "    X = df[features]\n",
    "    y = df['Target']\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Initialize the selected model\n",
    "    if model_name == 'Linear Regression':\n",
    "        final_model = LinearRegression()\n",
    "    elif model_name == 'Random Forest':\n",
    "        final_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    elif model_name == 'Gradient Boosting':\n",
    "        final_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "    elif model_name == 'XGBoost':\n",
    "        final_model = xgb.XGBRegressor(n_estimators=100, random_state=42, objective='reg:squarederror')\n",
    "    \n",
    "    # Train the model\n",
    "    final_model.fit(X_scaled, y)\n",
    "    \n",
    "    # Save model and scaler\n",
    "    joblib.dump(final_model, 'stock_prediction_model.pkl')\n",
    "    joblib.dump(scaler, 'feature_scaler.pkl')\n",
    "    \n",
    "    # Save feature list\n",
    "    with open('selected_features.txt', 'w') as f:\n",
    "        for feature in features:\n",
    "            f.write(f\"{feature}\\n\")\n",
    "    \n",
    "    return final_model, scaler\n",
    "# Modify the generate_predictions function to work without test data\n",
    "def generate_predictions(model, scaler, features, df, forecast_days=5):\n",
    "    print(\"\\nGenerating predictions for future days...\")\n",
    "    \n",
    "    # Use the last available data to predict future prices\n",
    "    latest_data = df.iloc[-1:].copy()\n",
    "    \n",
    "    # Create a dataframe for future dates\n",
    "    future_dates = [latest_data.index[0] + pd.Timedelta(days=i) for i in range(1, forecast_days+1)]\n",
    "    future_df = pd.DataFrame(index=future_dates)\n",
    "    \n",
    "    # Initialize with the latest values\n",
    "    for col in df.columns:\n",
    "        if col in ['Target']:\n",
    "            continue\n",
    "        future_df[col] = latest_data[col].values[0]\n",
    "    \n",
    "    # Make predictions day by day\n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(forecast_days):\n",
    "        # Prepare features for the current day\n",
    "        X = future_df.iloc[i:i+1][features]\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = scaler.transform(X)\n",
    "        \n",
    "        # Make prediction\n",
    "        predicted_price = model.predict(X_scaled)[0]\n",
    "        predictions.append(predicted_price)\n",
    "        \n",
    "        # Update the dataframe for the next day if needed\n",
    "        if i < forecast_days - 1:\n",
    "            # Update close price for the next day\n",
    "            future_df.iloc[i+1, future_df.columns.get_loc('Close')] = predicted_price\n",
    "            \n",
    "            # Update other features based on the new close price\n",
    "            # This is a simplified approach - in a real scenario, you would need\n",
    "            # to update all relevant technical indicators\n",
    "            if 'MA5' in future_df.columns:\n",
    "                # Example: Update 5-day moving average\n",
    "                ma5_values = list(df['Close'].iloc[-4:].values) + list(future_df['Close'].iloc[:i+1].values)\n",
    "                future_df.iloc[i+1, future_df.columns.get_loc('MA5')] = sum(ma5_values) / 5\n",
    "    \n",
    "    # Create a dataframe with the predictions\n",
    "    pred_df = pd.DataFrame({\n",
    "        'Date': future_dates,\n",
    "        'Predicted_Close': predictions\n",
    "    })\n",
    "    pred_df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Save predictions to CSV\n",
    "    pred_df.to_csv('stock_predictions.csv')\n",
    "    \n",
    "    print(f\"Predictions saved to 'stock_predictions.csv'\")\n",
    "    return pred_df\n",
    "\n",
    "# 9. Write README file\n",
    "def write_readme(model_name, performance, features):\n",
    "    print(\"\\nGenerating README file...\")\n",
    "    \n",
    "    with open('README.md', 'w') as f:\n",
    "        f.write(\"# Stock Price Prediction Model\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Overview\\n\")\n",
    "        f.write(\"This project implements a machine learning model to predict stock prices 5 trading days into the future. \")\n",
    "        f.write(\"The solution includes comprehensive data analysis, feature engineering, model selection, and trading strategy evaluation.\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Approach\\n\")\n",
    "        f.write(\"1. **Data Preprocessing**: Handled missing values and prepared the time series data.\\n\")\n",
    "        f.write(\"2. **Exploratory Data Analysis**: Analyzed trends, distributions, and correlations in the stock data.\\n\")\n",
    "        f.write(\"3. **Feature Engineering**: Created technical indicators, moving averages, and other predictive features.\\n\")\n",
    "        f.write(\"4. **Model Selection**: Compared multiple models including Linear Regression, Random Forest, Gradient Boosting, and XGBoost.\\n\")\n",
    "        f.write(\"5. **Trading Strategy**: Implemented and evaluated a simulated trading strategy based on model predictions.\\n\\n\")\n",
    "        \n",
    "        f.write(f\"## Selected Model: {model_name}\\n\")\n",
    "        f.write(f\"The {model_name} model was selected based on its superior performance in both statistical accuracy and trading performance.\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Performance Metrics\\n\")\n",
    "        f.write(f\"- Total Return: {performance['Total Return']:.4f}\\n\")\n",
    "        f.write(f\"- Market Return: {performance['Market Return']:.4f}\\n\")\n",
    "        f.write(f\"- Annualized Return: {performance['Annualized Return']:.4f}\\n\")\n",
    "        f.write(f\"- Sharpe Ratio: {performance['Sharpe Ratio']:.4f}\\n\")\n",
    "        f.write(f\"- Maximum Drawdown: {performance['Maximum Drawdown']:.4f}\\n\")\n",
    "        f.write(f\"- Win Rate: {performance['Win Rate']:.4f}\\n\")\n",
    "        f.write(f\"- Direction Accuracy: {performance['Direction Accuracy']:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Key Features\\n\")\n",
    "        for i, feature in enumerate(features[:10], 1):\n",
    "            f.write(f\"{i}. {feature}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"## Limitations and Future Improvements\\n\")\n",
    "        f.write(\"1. **Data Limitations**: The model relies solely on price and volume data. Incorporating fundamental data, market sentiment, and macroeconomic indicators could improve performance.\\n\")\n",
    "        f.write(\"2. **Model Complexity**: More sophisticated models like LSTM networks or ensemble methods could potentially capture more complex patterns.\\n\")\n",
    "        f.write(\"3. **Trading Strategy**: The current strategy is simplistic. More advanced position sizing, risk management, and portfolio optimization could enhance returns.\\n\")\n",
    "        f.write(\"4. **Market Regimes**: The model doesn't explicitly account for different market regimes (bull, bear, sideways). Developing regime-specific models could be beneficial.\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Usage Instructions\\n\")\n",
    "        f.write(\"1. Ensure all dependencies are installed: `pip install -r requirements.txt`\\n\")\n",
    "        f.write(\"2. To make predictions on new data: `python predict.py --input new_data.csv`\\n\")\n",
    "        f.write(\"3. Model files (`stock_prediction_model.pkl` and `feature_scaler.pkl`) are included for direct use.\\n\")\n",
    "    \n",
    "    print(\"README.md generated successfully\")\n",
    "\n",
    "# Modify the main function to use the updated generate_predictions function\n",
    "def main():\n",
    "    print(\"Stock Price Prediction Pipeline\\n\" + \"=\"*30)\n",
    "    try:\n",
    "        # Define file path (update with your actual file path)\n",
    "        file_path = '/kaggle/input/stock-data-analysis/question4-stock-data.csv'  \n",
    "        \n",
    "        # Step 1: Load and preprocess data\n",
    "        df = load_and_preprocess_data(file_path)\n",
    "        \n",
    "        # Initial check for problematic values\n",
    "        check_for_problematic_values(df, \"after loading data\")\n",
    "        \n",
    "        # Step 2: Perform EDA\n",
    "        df = perform_eda(df)\n",
    "        \n",
    "        # Step 3: Engineer features\n",
    "        df = engineer_features(df)\n",
    "        \n",
    "        # Replace any remaining infinite values\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # Final safety check\n",
    "        if df.empty:\n",
    "            raise ValueError(\"All data was filtered out during preprocessing. Check your dataset.\")\n",
    "        \n",
    "        # Step 4: Select important features\n",
    "        features = select_features(df)\n",
    "        \n",
    "        # Step 5: Train and evaluate models\n",
    "        best_model_name, best_model, results = train_and_evaluate_models(df, features)\n",
    "        \n",
    "        # Step 6: Evaluate trading strategy\n",
    "        performance, strategy_df = evaluate_trading_strategy(df, best_model_name, best_model, features)\n",
    "        \n",
    "        # Step 7: Train final model\n",
    "        final_model, scaler = train_final_model(df, best_model_name, features)\n",
    "        \n",
    "        # Step 8: Generate predictions for future days\n",
    "        predictions = generate_predictions(final_model, scaler, features, df)\n",
    "        \n",
    "        # Step 9: Write README\n",
    "        write_readme(best_model_name, performance, features)\n",
    "        \n",
    "        print(\"\\nStock prediction pipeline completed successfully!\")\n",
    "        print(\"Check the generated files for results and visualizations.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main pipeline: {e}\")\n",
    "        print(\"\\nTroubleshooting suggestions:\")\n",
    "        print(\"1. Check your input data for missing values, zeros, or extreme outliers\")\n",
    "        print(\"2. Try preprocessing your data to handle outliers\")\n",
    "        print(\"3. Ensure your date column is properly formatted\")\n",
    "        print(\"4. If the error persists, you might need to modify feature engineering to avoid division by zero\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6821667,
     "sourceId": 10964424,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 158.496914,
   "end_time": "2025-03-09T17:24:25.815925",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-09T17:21:47.319011",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
